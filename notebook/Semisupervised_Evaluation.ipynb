{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDatX-S2361T",
        "outputId": "a8e7bc72-b5c5-40cd-d45f-7f1f2b3825fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %cd '/content/drive/MyDrive/ecehw/project'\n",
        "%cd '/content/drive/MyDrive/MIDS/ECE661/self-supervised'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l8fb6STpDZs_",
        "outputId": "b916670d-7878-4b81-ad51-ba8ade68b762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/MIDS/ECE661/self-supervised\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os, sys\n",
        "\n",
        "\n",
        "from resnet import ResNet18_pred, MLP, Block\n",
        "\n",
        "# from tools.dataset import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2-fL4SNZvXz",
        "outputId": "cf68ef71-d8bc-4db1-a31c-5c9c3e3c62be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "trainset = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./data', train=True, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size = BATCH_SIZE, shuffle=True, )\n",
        "\n",
        "testset = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./data', train=False, download=True, transform=transforms.ToTensor()),\n",
        "    batch_size = BATCH_SIZE, shuffle=True, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxHtaLpUDLSj",
        "outputId": "edd752b4-1a03-406f-a6ac-415501b6e761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(trainset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfxnEUYzCYLl",
        "outputId": "97deb975-071e-4957-f672-355d1aa089c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "391"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_color_distortion(s:float=0.5):\n",
        "    \"\"\"\n",
        "    Function from the paper that create color distortion \n",
        "    s: float, the strength of color distortion, for CIFAR 10, the paper use 0.5\n",
        "    \"\"\"\n",
        "    color_jitter = transforms.ColorJitter(0.8*s, 0.8*s, 0.8*s, 0.2*s)\n",
        "    rnd_color_jitter = transforms.RandomApply([color_jitter], p=0.8)\n",
        "    rnd_gray = transforms.RandomGrayscale(p=0.2)\n",
        "    color_distort = transforms.Compose([rnd_color_jitter, rnd_gray])\n",
        "    return color_distort\n",
        "\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "            # make sure we're using PIL instead of tensor when doing other transform \n",
        "            transforms.ToPILImage(),\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            #transforms.GaussianBlur(23, sigma=(0.1, 2.0)), # CIFAR 10 doesn't use gaussian blur\n",
        "            #transforms.RandomResizedCrop(size=32,scale=(0.08,0.1),ratio=(0.75,1.33)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            #get_color_distortion(),\n",
        "            transforms.ToTensor(),\n",
        "            # the normalize numbers are from previous assignment\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])\n",
        "\n",
        "linear_eval_transform = transforms.Compose([\n",
        "            #transforms.GaussianBlur(23, sigma=(0.1, 2.0)), # CIFAR 10 doesn't use gaussian blur\n",
        "            transforms.RandomCrop(32, 4),\n",
        "            #transforms.RandomResizedCrop(size=32,scale=(0.08,0.1),ratio=(0.75,1.33)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            #get_color_distortion(),\n",
        "            transforms.ToTensor(),\n",
        "            # the normalize numbers are from previous assignment\n",
        "            transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "            ])"
      ],
      "metadata": {
        "id": "4zmj2iRhOHPO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0 \n",
        "      for data, targets in trainset:           \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "bFE_9hHoybT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_200_epochs(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      if epoch == 100 or epoch == 150:\n",
        "        current_learning_rate = current_learning_rate * 0.1\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0 \n",
        "      for data, targets in trainset:           \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "pJ6dKWW7bUWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_10_percent(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0 \n",
        "      for batch_ind, (data, targets) in enumerate(trainset):\n",
        "          if batch_ind == 40:\n",
        "              break         \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "gm1LT0KAIQ3c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_1_percent(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0 \n",
        "      for batch_ind, (data, targets) in enumerate(trainset):\n",
        "          if batch_ind == 4:\n",
        "              break         \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "6_Urj2fIDeFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_200_epochs_10_p(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      if epoch == 100 or epoch == 150:\n",
        "        current_learning_rate = current_learning_rate * 0.1\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0\n",
        "      for batch_ind, (data, targets) in enumerate(trainset):\n",
        "          if batch_ind == 40:\n",
        "              break            \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "5lWuBWiafBKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch_200_epochs_1_p(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      if epoch == 100 or epoch == 150:\n",
        "        current_learning_rate = current_learning_rate * 0.1\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = current_learning_rate\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0\n",
        "      for batch_ind, (data, targets) in enumerate(trainset):\n",
        "          if batch_ind == 4:\n",
        "              break            \n",
        "          data = data.to(device) \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(data)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          # scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc"
      ],
      "metadata": {
        "id": "x6kH7SYadA_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "def train_epoch(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename):\n",
        "  LOSSES = []\n",
        "  OPTIM_LOSS = float('inf')\n",
        "  for epoch in range(epoch):\n",
        "      cost = 0\n",
        "      correct_examples = 0 \n",
        "      total_examples = 0 \n",
        "      for data, targets in trainset:\n",
        "          total_tensor = total_tensor.to(device)\n",
        "          targets = targets.repeat_interleave(2)  \n",
        "          targets = targets.to(device)\n",
        "          # pass  into the model \n",
        "          outputs = net(total_tensor)\n",
        "          # calculate loss \n",
        "          # loss = compute_loss(yhat, 0.5)\n",
        "          #outputs = nn.Softmax(dim = 1)(outputs)\n",
        "          loss = criterion(outputs, targets)\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          scheduler.step()\n",
        "          cost += loss.item()\n",
        "\n",
        "          # output accuracy \n",
        "          _, predicted = torch.max(outputs.data, 1)\n",
        "          correct_examples += (predicted == targets).sum().item()\n",
        "          total_examples += targets.size(0)\n",
        "          \n",
        "\n",
        "      avg_loss = cost / len(trainset)\n",
        "      LOSSES.append(avg_loss)\n",
        "      if avg_loss < OPTIM_LOSS:\n",
        "          OPTIM_LOSS = avg_loss\n",
        "          torch.save(net.state_dict(), filename)\n",
        "      print(\"Epoch: {}, Average Loss {:f}\".format(epoch, avg_loss))\n",
        "      avg_acc = correct_examples / total_examples\n",
        "      print(\"Training accuracy: %.4f\" % (avg_acc))\n",
        "  return LOSSES, avg_acc\n",
        "  \"\"\""
      ],
      "metadata": {
        "id": "GYhiJXQNGSsl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clear cuda memory cache\n",
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPNQRVWjNF60",
        "outputId": "ea0c9657-3a6d-40d4-f1c0-792763d113ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1412"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainset = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10('./data', train=True, download=True, transform=linear_eval_transform),\n",
        "    batch_size = BATCH_SIZE, shuffle=True, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mJt3VlD8wWjs",
        "outputId": "9be15e3b-bd3c-4845-e235-d067d3e1247a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch(NET, 50, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')\n",
        "\n"
      ],
      "metadata": {
        "id": "WVUirP4dI6Y9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cad85c3-dbdf-4b99-ead8-2e0af98ebece"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 1.900710\n",
            "Training accuracy: 0.3245\n",
            "Epoch: 1, Average Loss 1.388787\n",
            "Training accuracy: 0.4893\n",
            "Epoch: 2, Average Loss 1.144791\n",
            "Training accuracy: 0.5875\n",
            "Epoch: 3, Average Loss 0.928661\n",
            "Training accuracy: 0.6691\n",
            "Epoch: 4, Average Loss 0.791151\n",
            "Training accuracy: 0.7194\n",
            "Epoch: 5, Average Loss 0.685247\n",
            "Training accuracy: 0.7577\n",
            "Epoch: 6, Average Loss 0.593766\n",
            "Training accuracy: 0.7898\n",
            "Epoch: 7, Average Loss 0.516155\n",
            "Training accuracy: 0.8197\n",
            "Epoch: 8, Average Loss 0.450885\n",
            "Training accuracy: 0.8432\n",
            "Epoch: 9, Average Loss 0.396796\n",
            "Training accuracy: 0.8627\n",
            "Epoch: 10, Average Loss 0.355516\n",
            "Training accuracy: 0.8750\n",
            "Epoch: 11, Average Loss 0.318358\n",
            "Training accuracy: 0.8906\n",
            "Epoch: 12, Average Loss 0.286276\n",
            "Training accuracy: 0.9020\n",
            "Epoch: 13, Average Loss 0.258276\n",
            "Training accuracy: 0.9129\n",
            "Epoch: 14, Average Loss 0.242215\n",
            "Training accuracy: 0.9175\n",
            "Epoch: 15, Average Loss 0.230878\n",
            "Training accuracy: 0.9214\n",
            "Epoch: 16, Average Loss 0.216410\n",
            "Training accuracy: 0.9265\n",
            "Epoch: 17, Average Loss 0.202445\n",
            "Training accuracy: 0.9310\n",
            "Epoch: 18, Average Loss 0.193074\n",
            "Training accuracy: 0.9345\n",
            "Epoch: 19, Average Loss 0.183698\n",
            "Training accuracy: 0.9382\n",
            "Epoch: 20, Average Loss 0.178843\n",
            "Training accuracy: 0.9398\n",
            "Epoch: 21, Average Loss 0.167115\n",
            "Training accuracy: 0.9433\n",
            "Epoch: 22, Average Loss 0.162511\n",
            "Training accuracy: 0.9458\n",
            "Epoch: 23, Average Loss 0.160779\n",
            "Training accuracy: 0.9467\n",
            "Epoch: 24, Average Loss 0.152626\n",
            "Training accuracy: 0.9489\n",
            "Epoch: 25, Average Loss 0.147219\n",
            "Training accuracy: 0.9505\n",
            "Epoch: 26, Average Loss 0.141111\n",
            "Training accuracy: 0.9530\n",
            "Epoch: 27, Average Loss 0.143563\n",
            "Training accuracy: 0.9520\n",
            "Epoch: 28, Average Loss 0.134479\n",
            "Training accuracy: 0.9556\n",
            "Epoch: 29, Average Loss 0.134538\n",
            "Training accuracy: 0.9571\n",
            "Epoch: 30, Average Loss 0.134961\n",
            "Training accuracy: 0.9546\n",
            "Epoch: 31, Average Loss 0.130096\n",
            "Training accuracy: 0.9577\n",
            "Epoch: 32, Average Loss 0.129878\n",
            "Training accuracy: 0.9570\n",
            "Epoch: 33, Average Loss 0.127111\n",
            "Training accuracy: 0.9573\n",
            "Epoch: 34, Average Loss 0.138674\n",
            "Training accuracy: 0.9538\n",
            "Epoch: 35, Average Loss 0.123724\n",
            "Training accuracy: 0.9586\n",
            "Epoch: 36, Average Loss 0.121284\n",
            "Training accuracy: 0.9610\n",
            "Epoch: 37, Average Loss 0.126305\n",
            "Training accuracy: 0.9595\n",
            "Epoch: 38, Average Loss 0.128996\n",
            "Training accuracy: 0.9576\n",
            "Epoch: 39, Average Loss 0.119735\n",
            "Training accuracy: 0.9618\n",
            "Epoch: 40, Average Loss 0.123850\n",
            "Training accuracy: 0.9600\n",
            "Epoch: 41, Average Loss 0.114215\n",
            "Training accuracy: 0.9627\n",
            "Epoch: 42, Average Loss 0.113323\n",
            "Training accuracy: 0.9627\n",
            "Epoch: 43, Average Loss 0.121467\n",
            "Training accuracy: 0.9597\n",
            "Epoch: 44, Average Loss 0.111966\n",
            "Training accuracy: 0.9629\n",
            "Epoch: 45, Average Loss 0.117121\n",
            "Training accuracy: 0.9623\n",
            "Epoch: 46, Average Loss 0.108035\n",
            "Training accuracy: 0.9649\n",
            "Epoch: 47, Average Loss 0.117088\n",
            "Training accuracy: 0.9609\n",
            "Epoch: 48, Average Loss 0.116055\n",
            "Training accuracy: 0.9614\n",
            "Epoch: 49, Average Loss 0.125635\n",
            "Training accuracy: 0.9588\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([1.9007104921828755,\n",
              "  1.388786952818751,\n",
              "  1.1447914576591434,\n",
              "  0.9286606113624085,\n",
              "  0.7911506825700745,\n",
              "  0.6852468516668091,\n",
              "  0.5937656249536578,\n",
              "  0.5161551413938518,\n",
              "  0.45088546664056267,\n",
              "  0.3967959358335456,\n",
              "  0.35551648855666673,\n",
              "  0.3183584355789682,\n",
              "  0.28627607256860077,\n",
              "  0.25827578506658755,\n",
              "  0.242215142139922,\n",
              "  0.23087784405940634,\n",
              "  0.216409504785181,\n",
              "  0.20244475140157717,\n",
              "  0.19307350734596515,\n",
              "  0.18369787482692457,\n",
              "  0.17884255112970576,\n",
              "  0.1671150904053541,\n",
              "  0.16251146834334143,\n",
              "  0.16077928510411163,\n",
              "  0.15262614526902624,\n",
              "  0.14721898949416853,\n",
              "  0.14111122828634345,\n",
              "  0.14356312485854797,\n",
              "  0.13447915190530707,\n",
              "  0.13453796235344295,\n",
              "  0.1349609971351331,\n",
              "  0.1300956716194101,\n",
              "  0.12987835499484215,\n",
              "  0.1271114190635474,\n",
              "  0.13867399687199947,\n",
              "  0.12372361155002928,\n",
              "  0.12128425184208566,\n",
              "  0.12630545232168702,\n",
              "  0.12899630344318003,\n",
              "  0.11973524606927201,\n",
              "  0.12385040808878743,\n",
              "  0.11421496804108096,\n",
              "  0.11332332612017688,\n",
              "  0.12146663753425374,\n",
              "  0.11196563629638356,\n",
              "  0.11712125766917568,\n",
              "  0.10803512766804842,\n",
              "  0.11708814210599036,\n",
              "  0.11605480656770946,\n",
              "  0.12563487971225357],\n",
              " 0.95884)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "\n",
        "cost = 0\n",
        "total, correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, label in testset:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        y_pred = NET(data)\n",
        "        # calculate loss \n",
        "        loss = F.cross_entropy(y_pred, label)\n",
        "        cost += loss.item()\n",
        "\n",
        "        total += len(label)\n",
        "        y_hat = torch.argmax(y_pred, dim=1)\n",
        "        correct += torch.sum(torch.eq(y_hat, label)).item()\n",
        "\n",
        "avg_loss_test = cost / len(testset)\n",
        "print(\"The test accuracy is:\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2LhY9c7fG-I1",
        "outputId": "63f1ffe0-072b-4622-cf81-83a228828cd5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.8123\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch_10_percent(NET, 50, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bgcPQaTYCj6K",
        "outputId": "5f97a34e-9d42-4c3c-abaa-5f5c3a8531ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 0.302375\n",
            "Training accuracy: 0.1836\n",
            "Epoch: 1, Average Loss 0.203737\n",
            "Training accuracy: 0.2516\n",
            "Epoch: 2, Average Loss 0.189344\n",
            "Training accuracy: 0.3059\n",
            "Epoch: 3, Average Loss 0.179501\n",
            "Training accuracy: 0.3408\n",
            "Epoch: 4, Average Loss 0.172024\n",
            "Training accuracy: 0.3707\n",
            "Epoch: 5, Average Loss 0.168445\n",
            "Training accuracy: 0.3893\n",
            "Epoch: 6, Average Loss 0.169469\n",
            "Training accuracy: 0.3773\n",
            "Epoch: 7, Average Loss 0.170274\n",
            "Training accuracy: 0.3758\n",
            "Epoch: 8, Average Loss 0.167580\n",
            "Training accuracy: 0.3844\n",
            "Epoch: 9, Average Loss 0.162807\n",
            "Training accuracy: 0.4078\n",
            "Epoch: 10, Average Loss 0.157757\n",
            "Training accuracy: 0.4279\n",
            "Epoch: 11, Average Loss 0.152965\n",
            "Training accuracy: 0.4465\n",
            "Epoch: 12, Average Loss 0.142807\n",
            "Training accuracy: 0.4820\n",
            "Epoch: 13, Average Loss 0.135172\n",
            "Training accuracy: 0.5221\n",
            "Epoch: 14, Average Loss 0.127058\n",
            "Training accuracy: 0.5367\n",
            "Epoch: 15, Average Loss 0.125007\n",
            "Training accuracy: 0.5484\n",
            "Epoch: 16, Average Loss 0.127594\n",
            "Training accuracy: 0.5404\n",
            "Epoch: 17, Average Loss 0.134642\n",
            "Training accuracy: 0.5166\n",
            "Epoch: 18, Average Loss 0.137529\n",
            "Training accuracy: 0.5127\n",
            "Epoch: 19, Average Loss 0.134143\n",
            "Training accuracy: 0.5227\n",
            "Epoch: 20, Average Loss 0.135710\n",
            "Training accuracy: 0.5203\n",
            "Epoch: 21, Average Loss 0.127988\n",
            "Training accuracy: 0.5494\n",
            "Epoch: 22, Average Loss 0.116211\n",
            "Training accuracy: 0.5873\n",
            "Epoch: 23, Average Loss 0.106790\n",
            "Training accuracy: 0.6258\n",
            "Epoch: 24, Average Loss 0.100061\n",
            "Training accuracy: 0.6494\n",
            "Epoch: 25, Average Loss 0.100650\n",
            "Training accuracy: 0.6395\n",
            "Epoch: 26, Average Loss 0.103331\n",
            "Training accuracy: 0.6387\n",
            "Epoch: 27, Average Loss 0.106046\n",
            "Training accuracy: 0.6305\n",
            "Epoch: 28, Average Loss 0.113057\n",
            "Training accuracy: 0.5988\n",
            "Epoch: 29, Average Loss 0.117110\n",
            "Training accuracy: 0.5949\n",
            "Epoch: 30, Average Loss 0.111062\n",
            "Training accuracy: 0.6092\n",
            "Epoch: 31, Average Loss 0.104531\n",
            "Training accuracy: 0.6307\n",
            "Epoch: 32, Average Loss 0.095140\n",
            "Training accuracy: 0.6783\n",
            "Epoch: 33, Average Loss 0.084345\n",
            "Training accuracy: 0.7031\n",
            "Epoch: 34, Average Loss 0.079314\n",
            "Training accuracy: 0.7242\n",
            "Epoch: 35, Average Loss 0.078529\n",
            "Training accuracy: 0.7230\n",
            "Epoch: 36, Average Loss 0.080360\n",
            "Training accuracy: 0.7137\n",
            "Epoch: 37, Average Loss 0.086018\n",
            "Training accuracy: 0.7018\n",
            "Epoch: 38, Average Loss 0.095827\n",
            "Training accuracy: 0.6678\n",
            "Epoch: 39, Average Loss 0.095416\n",
            "Training accuracy: 0.6676\n",
            "Epoch: 40, Average Loss 0.092309\n",
            "Training accuracy: 0.6824\n",
            "Epoch: 41, Average Loss 0.092723\n",
            "Training accuracy: 0.6783\n",
            "Epoch: 42, Average Loss 0.078027\n",
            "Training accuracy: 0.7293\n",
            "Epoch: 43, Average Loss 0.071163\n",
            "Training accuracy: 0.7646\n",
            "Epoch: 44, Average Loss 0.063264\n",
            "Training accuracy: 0.7797\n",
            "Epoch: 45, Average Loss 0.065487\n",
            "Training accuracy: 0.7791\n",
            "Epoch: 46, Average Loss 0.065775\n",
            "Training accuracy: 0.7703\n",
            "Epoch: 47, Average Loss 0.071447\n",
            "Training accuracy: 0.7539\n",
            "Epoch: 48, Average Loss 0.077276\n",
            "Training accuracy: 0.7299\n",
            "Epoch: 49, Average Loss 0.082692\n",
            "Training accuracy: 0.7119\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.30237480289186053,\n",
              "  0.20373696774777855,\n",
              "  0.18934429941884698,\n",
              "  0.17950100636543215,\n",
              "  0.17202385337761297,\n",
              "  0.16844459964186334,\n",
              "  0.16946859829261174,\n",
              "  0.17027405765660278,\n",
              "  0.1675801865585015,\n",
              "  0.1628070664222893,\n",
              "  0.15775749476059622,\n",
              "  0.15296481850811894,\n",
              "  0.14280695683511016,\n",
              "  0.1351716786699222,\n",
              "  0.1270580608826464,\n",
              "  0.1250072159730565,\n",
              "  0.12759434353665014,\n",
              "  0.13464237539969443,\n",
              "  0.13752944146275825,\n",
              "  0.1341426195695882,\n",
              "  0.1357095323865066,\n",
              "  0.1279877158991821,\n",
              "  0.11621077240580488,\n",
              "  0.10679040723444556,\n",
              "  0.10006094451450631,\n",
              "  0.10064958596168577,\n",
              "  0.1033307172148429,\n",
              "  0.10604639141760824,\n",
              "  0.11305741275972722,\n",
              "  0.11710980618396379,\n",
              "  0.11106205184746276,\n",
              "  0.10453143586283145,\n",
              "  0.09514034175506943,\n",
              "  0.08434464010741095,\n",
              "  0.07931350015313424,\n",
              "  0.0785290466245178,\n",
              "  0.0803597069457364,\n",
              "  0.08601844310760498,\n",
              "  0.09582682628460858,\n",
              "  0.095416425439098,\n",
              "  0.09230934902835075,\n",
              "  0.09272275922243553,\n",
              "  0.07802665858622403,\n",
              "  0.07116341590881348,\n",
              "  0.06326370653898819,\n",
              "  0.06548701809800189,\n",
              "  0.06577494458469284,\n",
              "  0.07144705551054777,\n",
              "  0.07727588747468446,\n",
              "  0.082691921145105],\n",
              " 0.7119140625)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "\n",
        "cost = 0\n",
        "total, correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, label in testset:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        y_pred = NET(data)\n",
        "        # calculate loss \n",
        "        loss = F.cross_entropy(y_pred, label)\n",
        "        cost += loss.item()\n",
        "\n",
        "        total += len(label)\n",
        "        y_hat = torch.argmax(y_pred, dim=1)\n",
        "        correct += torch.sum(torch.eq(y_hat, label)).item()\n",
        "\n",
        "avg_loss_test = cost / len(testset)\n",
        "print(\"The test accuracy is:\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FUDe2PajFP28",
        "outputId": "3ec38ea2-751f-4656-f611-1a02596aee5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.5022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch_1_percent(NET, 50, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MqIatAr5Jbbl",
        "outputId": "51aba3f0-f81b-4eff-fca3-08dbbb8d5751"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 0.040876\n",
            "Training accuracy: 0.1172\n",
            "Epoch: 1, Average Loss 0.049863\n",
            "Training accuracy: 0.1172\n",
            "Epoch: 2, Average Loss 0.033887\n",
            "Training accuracy: 0.1211\n",
            "Epoch: 3, Average Loss 0.031251\n",
            "Training accuracy: 0.1016\n",
            "Epoch: 4, Average Loss 0.026879\n",
            "Training accuracy: 0.1836\n",
            "Epoch: 5, Average Loss 0.023455\n",
            "Training accuracy: 0.1777\n",
            "Epoch: 6, Average Loss 0.027356\n",
            "Training accuracy: 0.1875\n",
            "Epoch: 7, Average Loss 0.026833\n",
            "Training accuracy: 0.1582\n",
            "Epoch: 8, Average Loss 0.026743\n",
            "Training accuracy: 0.1855\n",
            "Epoch: 9, Average Loss 0.023019\n",
            "Training accuracy: 0.1934\n",
            "Epoch: 10, Average Loss 0.022011\n",
            "Training accuracy: 0.2168\n",
            "Epoch: 11, Average Loss 0.021772\n",
            "Training accuracy: 0.1836\n",
            "Epoch: 12, Average Loss 0.021164\n",
            "Training accuracy: 0.2344\n",
            "Epoch: 13, Average Loss 0.021082\n",
            "Training accuracy: 0.2246\n",
            "Epoch: 14, Average Loss 0.020786\n",
            "Training accuracy: 0.2402\n",
            "Epoch: 15, Average Loss 0.021595\n",
            "Training accuracy: 0.2168\n",
            "Epoch: 16, Average Loss 0.020902\n",
            "Training accuracy: 0.2383\n",
            "Epoch: 17, Average Loss 0.020559\n",
            "Training accuracy: 0.2285\n",
            "Epoch: 18, Average Loss 0.021048\n",
            "Training accuracy: 0.2383\n",
            "Epoch: 19, Average Loss 0.019911\n",
            "Training accuracy: 0.2871\n",
            "Epoch: 20, Average Loss 0.020236\n",
            "Training accuracy: 0.2734\n",
            "Epoch: 21, Average Loss 0.020011\n",
            "Training accuracy: 0.2910\n",
            "Epoch: 22, Average Loss 0.019684\n",
            "Training accuracy: 0.2441\n",
            "Epoch: 23, Average Loss 0.019627\n",
            "Training accuracy: 0.2969\n",
            "Epoch: 24, Average Loss 0.019259\n",
            "Training accuracy: 0.2891\n",
            "Epoch: 25, Average Loss 0.018890\n",
            "Training accuracy: 0.3145\n",
            "Epoch: 26, Average Loss 0.019520\n",
            "Training accuracy: 0.2910\n",
            "Epoch: 27, Average Loss 0.018629\n",
            "Training accuracy: 0.3398\n",
            "Epoch: 28, Average Loss 0.019125\n",
            "Training accuracy: 0.3008\n",
            "Epoch: 29, Average Loss 0.018880\n",
            "Training accuracy: 0.2734\n",
            "Epoch: 30, Average Loss 0.018572\n",
            "Training accuracy: 0.3477\n",
            "Epoch: 31, Average Loss 0.018150\n",
            "Training accuracy: 0.3340\n",
            "Epoch: 32, Average Loss 0.018424\n",
            "Training accuracy: 0.3242\n",
            "Epoch: 33, Average Loss 0.018351\n",
            "Training accuracy: 0.3691\n",
            "Epoch: 34, Average Loss 0.018595\n",
            "Training accuracy: 0.3105\n",
            "Epoch: 35, Average Loss 0.018255\n",
            "Training accuracy: 0.3359\n",
            "Epoch: 36, Average Loss 0.018164\n",
            "Training accuracy: 0.3164\n",
            "Epoch: 37, Average Loss 0.017847\n",
            "Training accuracy: 0.3398\n",
            "Epoch: 38, Average Loss 0.017359\n",
            "Training accuracy: 0.3457\n",
            "Epoch: 39, Average Loss 0.018000\n",
            "Training accuracy: 0.3535\n",
            "Epoch: 40, Average Loss 0.017513\n",
            "Training accuracy: 0.3516\n",
            "Epoch: 41, Average Loss 0.016679\n",
            "Training accuracy: 0.3789\n",
            "Epoch: 42, Average Loss 0.017448\n",
            "Training accuracy: 0.3594\n",
            "Epoch: 43, Average Loss 0.017697\n",
            "Training accuracy: 0.3652\n",
            "Epoch: 44, Average Loss 0.016647\n",
            "Training accuracy: 0.3867\n",
            "Epoch: 45, Average Loss 0.017345\n",
            "Training accuracy: 0.3711\n",
            "Epoch: 46, Average Loss 0.017860\n",
            "Training accuracy: 0.3516\n",
            "Epoch: 47, Average Loss 0.017743\n",
            "Training accuracy: 0.3945\n",
            "Epoch: 48, Average Loss 0.017645\n",
            "Training accuracy: 0.3613\n",
            "Epoch: 49, Average Loss 0.017583\n",
            "Training accuracy: 0.3574\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.040875604390488256,\n",
              "  0.049863398227545305,\n",
              "  0.033886661919791373,\n",
              "  0.03125099757748187,\n",
              "  0.02687902279826991,\n",
              "  0.023454609429439926,\n",
              "  0.02735624227987226,\n",
              "  0.02683255251716165,\n",
              "  0.026743086402678428,\n",
              "  0.02301853697013367,\n",
              "  0.022010839808627468,\n",
              "  0.021771636460443288,\n",
              "  0.02116375567053285,\n",
              "  0.021082336335535853,\n",
              "  0.020786156434842083,\n",
              "  0.02159465731257368,\n",
              "  0.02090215896401564,\n",
              "  0.02055948805016325,\n",
              "  0.02104844827481243,\n",
              "  0.01991071633975524,\n",
              "  0.020235627813412406,\n",
              "  0.02001088049710559,\n",
              "  0.019683600996461367,\n",
              "  0.019627168355390544,\n",
              "  0.019259024763960974,\n",
              "  0.018890091220436195,\n",
              "  0.01952029067232176,\n",
              "  0.01862892897232719,\n",
              "  0.019124879251660593,\n",
              "  0.01887975903728124,\n",
              "  0.01857249998985349,\n",
              "  0.018149925924627983,\n",
              "  0.018423710027923975,\n",
              "  0.018350542963618207,\n",
              "  0.01859538634414868,\n",
              "  0.018254837721510007,\n",
              "  0.018164366407467582,\n",
              "  0.01784658432006836,\n",
              "  0.01735934028235238,\n",
              "  0.017999796306385714,\n",
              "  0.01751305745995563,\n",
              "  0.016679366226391414,\n",
              "  0.01744826675376014,\n",
              "  0.017696586411322473,\n",
              "  0.016647064472403366,\n",
              "  0.017344640648883324,\n",
              "  0.017859862893438706,\n",
              "  0.017742518878653836,\n",
              "  0.01764481573763406,\n",
              "  0.01758300190996331],\n",
              " 0.357421875)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "\n",
        "cost = 0\n",
        "total, correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, label in testset:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        y_pred = NET(data)\n",
        "        # calculate loss \n",
        "        loss = F.cross_entropy(y_pred, label)\n",
        "        cost += loss.item()\n",
        "\n",
        "        total += len(label)\n",
        "        y_hat = torch.argmax(y_pred, dim=1)\n",
        "        correct += torch.sum(torch.eq(y_hat, label)).item()\n",
        "\n",
        "avg_loss_test = cost / len(testset)\n",
        "print(\"The test accuracy is:\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09Xxir8BJtql",
        "outputId": "963bbf4d-f446-461c-c515-ad326af3e884"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.3716\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch_200_epochs(NET, 200, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Z26E4inTK4v5",
        "outputId": "09af264d-6742-45b3-f982-d88146bc6120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 1.809095\n",
            "Training accuracy: 0.3513\n",
            "Epoch: 1, Average Loss 1.283650\n",
            "Training accuracy: 0.5324\n",
            "Epoch: 2, Average Loss 0.983742\n",
            "Training accuracy: 0.6481\n",
            "Epoch: 3, Average Loss 0.803510\n",
            "Training accuracy: 0.7150\n",
            "Epoch: 4, Average Loss 0.659460\n",
            "Training accuracy: 0.7686\n",
            "Epoch: 5, Average Loss 0.539391\n",
            "Training accuracy: 0.8122\n",
            "Epoch: 6, Average Loss 0.465035\n",
            "Training accuracy: 0.8387\n",
            "Epoch: 7, Average Loss 0.407115\n",
            "Training accuracy: 0.8582\n",
            "Epoch: 8, Average Loss 0.363887\n",
            "Training accuracy: 0.8737\n",
            "Epoch: 9, Average Loss 0.335131\n",
            "Training accuracy: 0.8837\n",
            "Epoch: 10, Average Loss 0.293170\n",
            "Training accuracy: 0.8989\n",
            "Epoch: 11, Average Loss 0.282061\n",
            "Training accuracy: 0.9018\n",
            "Epoch: 12, Average Loss 0.260139\n",
            "Training accuracy: 0.9084\n",
            "Epoch: 13, Average Loss 0.242687\n",
            "Training accuracy: 0.9156\n",
            "Epoch: 14, Average Loss 0.232123\n",
            "Training accuracy: 0.9192\n",
            "Epoch: 15, Average Loss 0.226547\n",
            "Training accuracy: 0.9215\n",
            "Epoch: 16, Average Loss 0.210818\n",
            "Training accuracy: 0.9267\n",
            "Epoch: 17, Average Loss 0.207170\n",
            "Training accuracy: 0.9275\n",
            "Epoch: 18, Average Loss 0.200137\n",
            "Training accuracy: 0.9317\n",
            "Epoch: 19, Average Loss 0.199772\n",
            "Training accuracy: 0.9318\n",
            "Epoch: 20, Average Loss 0.195899\n",
            "Training accuracy: 0.9323\n",
            "Epoch: 21, Average Loss 0.194072\n",
            "Training accuracy: 0.9334\n",
            "Epoch: 22, Average Loss 0.183564\n",
            "Training accuracy: 0.9371\n",
            "Epoch: 23, Average Loss 0.186262\n",
            "Training accuracy: 0.9351\n",
            "Epoch: 24, Average Loss 0.192063\n",
            "Training accuracy: 0.9339\n",
            "Epoch: 25, Average Loss 0.180974\n",
            "Training accuracy: 0.9381\n",
            "Epoch: 26, Average Loss 0.185530\n",
            "Training accuracy: 0.9360\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-d35f8e128342>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mNET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mNET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mtrain_epoch_200_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNET\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCRITERION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOPTIMIZER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCURRENT_LR\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mSCHEDULER\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-40-1b634c5cde95>\u001b[0m in \u001b[0;36mtrain_epoch_200_epochs\u001b[0;34m(net, epoch, criterion, optimizer, current_learning_rate, scheduler, filename)\u001b[0m\n\u001b[1;32m     23\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m           \u001b[0;31m# scheduler.step()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m           \u001b[0mcost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m           \u001b[0;31m# output accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch_200_epochs_10_p(NET, 200, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q0HBZ3eXfKXb",
        "outputId": "186da927-82bb-47db-bb84-1ef00f579c7c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 0.294276\n",
            "Training accuracy: 0.1781\n",
            "Epoch: 1, Average Loss 0.202809\n",
            "Training accuracy: 0.2707\n",
            "Epoch: 2, Average Loss 0.184640\n",
            "Training accuracy: 0.3309\n",
            "Epoch: 3, Average Loss 0.176356\n",
            "Training accuracy: 0.3748\n",
            "Epoch: 4, Average Loss 0.168305\n",
            "Training accuracy: 0.3820\n",
            "Epoch: 5, Average Loss 0.157866\n",
            "Training accuracy: 0.4281\n",
            "Epoch: 6, Average Loss 0.155499\n",
            "Training accuracy: 0.4359\n",
            "Epoch: 7, Average Loss 0.150746\n",
            "Training accuracy: 0.4551\n",
            "Epoch: 8, Average Loss 0.149297\n",
            "Training accuracy: 0.4578\n",
            "Epoch: 9, Average Loss 0.142495\n",
            "Training accuracy: 0.4811\n",
            "Epoch: 10, Average Loss 0.138921\n",
            "Training accuracy: 0.5008\n",
            "Epoch: 11, Average Loss 0.133629\n",
            "Training accuracy: 0.5215\n",
            "Epoch: 12, Average Loss 0.130654\n",
            "Training accuracy: 0.5371\n",
            "Epoch: 13, Average Loss 0.122441\n",
            "Training accuracy: 0.5641\n",
            "Epoch: 14, Average Loss 0.121094\n",
            "Training accuracy: 0.5686\n",
            "Epoch: 15, Average Loss 0.116254\n",
            "Training accuracy: 0.5984\n",
            "Epoch: 16, Average Loss 0.113159\n",
            "Training accuracy: 0.6037\n",
            "Epoch: 17, Average Loss 0.110784\n",
            "Training accuracy: 0.6176\n",
            "Epoch: 18, Average Loss 0.101799\n",
            "Training accuracy: 0.6443\n",
            "Epoch: 19, Average Loss 0.105297\n",
            "Training accuracy: 0.6340\n",
            "Epoch: 20, Average Loss 0.100335\n",
            "Training accuracy: 0.6545\n",
            "Epoch: 21, Average Loss 0.098575\n",
            "Training accuracy: 0.6570\n",
            "Epoch: 22, Average Loss 0.094243\n",
            "Training accuracy: 0.6707\n",
            "Epoch: 23, Average Loss 0.093686\n",
            "Training accuracy: 0.6787\n",
            "Epoch: 24, Average Loss 0.089056\n",
            "Training accuracy: 0.6959\n",
            "Epoch: 25, Average Loss 0.088785\n",
            "Training accuracy: 0.6975\n",
            "Epoch: 26, Average Loss 0.086114\n",
            "Training accuracy: 0.7043\n",
            "Epoch: 27, Average Loss 0.082142\n",
            "Training accuracy: 0.7150\n",
            "Epoch: 28, Average Loss 0.082178\n",
            "Training accuracy: 0.7170\n",
            "Epoch: 29, Average Loss 0.075909\n",
            "Training accuracy: 0.7414\n",
            "Epoch: 30, Average Loss 0.072896\n",
            "Training accuracy: 0.7488\n",
            "Epoch: 31, Average Loss 0.072741\n",
            "Training accuracy: 0.7502\n",
            "Epoch: 32, Average Loss 0.072279\n",
            "Training accuracy: 0.7559\n",
            "Epoch: 33, Average Loss 0.071714\n",
            "Training accuracy: 0.7525\n",
            "Epoch: 34, Average Loss 0.070872\n",
            "Training accuracy: 0.7547\n",
            "Epoch: 35, Average Loss 0.064774\n",
            "Training accuracy: 0.7795\n",
            "Epoch: 36, Average Loss 0.062680\n",
            "Training accuracy: 0.7861\n",
            "Epoch: 37, Average Loss 0.062114\n",
            "Training accuracy: 0.7896\n",
            "Epoch: 38, Average Loss 0.059829\n",
            "Training accuracy: 0.7961\n",
            "Epoch: 39, Average Loss 0.058784\n",
            "Training accuracy: 0.8016\n",
            "Epoch: 40, Average Loss 0.058702\n",
            "Training accuracy: 0.8064\n",
            "Epoch: 41, Average Loss 0.058893\n",
            "Training accuracy: 0.7998\n",
            "Epoch: 42, Average Loss 0.054017\n",
            "Training accuracy: 0.8158\n",
            "Epoch: 43, Average Loss 0.053316\n",
            "Training accuracy: 0.8209\n",
            "Epoch: 44, Average Loss 0.053380\n",
            "Training accuracy: 0.8184\n",
            "Epoch: 45, Average Loss 0.049875\n",
            "Training accuracy: 0.8334\n",
            "Epoch: 46, Average Loss 0.052335\n",
            "Training accuracy: 0.8322\n",
            "Epoch: 47, Average Loss 0.045680\n",
            "Training accuracy: 0.8410\n",
            "Epoch: 48, Average Loss 0.049044\n",
            "Training accuracy: 0.8295\n",
            "Epoch: 49, Average Loss 0.049450\n",
            "Training accuracy: 0.8338\n",
            "Epoch: 50, Average Loss 0.048859\n",
            "Training accuracy: 0.8295\n",
            "Epoch: 51, Average Loss 0.045830\n",
            "Training accuracy: 0.8473\n",
            "Epoch: 52, Average Loss 0.045472\n",
            "Training accuracy: 0.8441\n",
            "Epoch: 53, Average Loss 0.043304\n",
            "Training accuracy: 0.8570\n",
            "Epoch: 54, Average Loss 0.042051\n",
            "Training accuracy: 0.8623\n",
            "Epoch: 55, Average Loss 0.043702\n",
            "Training accuracy: 0.8510\n",
            "Epoch: 56, Average Loss 0.041527\n",
            "Training accuracy: 0.8609\n",
            "Epoch: 57, Average Loss 0.041800\n",
            "Training accuracy: 0.8617\n",
            "Epoch: 58, Average Loss 0.041770\n",
            "Training accuracy: 0.8602\n",
            "Epoch: 59, Average Loss 0.041717\n",
            "Training accuracy: 0.8596\n",
            "Epoch: 60, Average Loss 0.037505\n",
            "Training accuracy: 0.8811\n",
            "Epoch: 61, Average Loss 0.038581\n",
            "Training accuracy: 0.8752\n",
            "Epoch: 62, Average Loss 0.039459\n",
            "Training accuracy: 0.8695\n",
            "Epoch: 63, Average Loss 0.037614\n",
            "Training accuracy: 0.8795\n",
            "Epoch: 64, Average Loss 0.036385\n",
            "Training accuracy: 0.8816\n",
            "Epoch: 65, Average Loss 0.036625\n",
            "Training accuracy: 0.8770\n",
            "Epoch: 66, Average Loss 0.036221\n",
            "Training accuracy: 0.8818\n",
            "Epoch: 67, Average Loss 0.037252\n",
            "Training accuracy: 0.8756\n",
            "Epoch: 68, Average Loss 0.033272\n",
            "Training accuracy: 0.8895\n",
            "Epoch: 69, Average Loss 0.034788\n",
            "Training accuracy: 0.8857\n",
            "Epoch: 70, Average Loss 0.034353\n",
            "Training accuracy: 0.8811\n",
            "Epoch: 71, Average Loss 0.032211\n",
            "Training accuracy: 0.8971\n",
            "Epoch: 72, Average Loss 0.034987\n",
            "Training accuracy: 0.8816\n",
            "Epoch: 73, Average Loss 0.032410\n",
            "Training accuracy: 0.8912\n",
            "Epoch: 74, Average Loss 0.032559\n",
            "Training accuracy: 0.8980\n",
            "Epoch: 75, Average Loss 0.030860\n",
            "Training accuracy: 0.8994\n",
            "Epoch: 76, Average Loss 0.031895\n",
            "Training accuracy: 0.8963\n",
            "Epoch: 77, Average Loss 0.029402\n",
            "Training accuracy: 0.9023\n",
            "Epoch: 78, Average Loss 0.033063\n",
            "Training accuracy: 0.8914\n",
            "Epoch: 79, Average Loss 0.031798\n",
            "Training accuracy: 0.8883\n",
            "Epoch: 80, Average Loss 0.032457\n",
            "Training accuracy: 0.8934\n",
            "Epoch: 81, Average Loss 0.028915\n",
            "Training accuracy: 0.9021\n",
            "Epoch: 82, Average Loss 0.030114\n",
            "Training accuracy: 0.9002\n",
            "Epoch: 83, Average Loss 0.030060\n",
            "Training accuracy: 0.9008\n",
            "Epoch: 84, Average Loss 0.029835\n",
            "Training accuracy: 0.9049\n",
            "Epoch: 85, Average Loss 0.026829\n",
            "Training accuracy: 0.9092\n",
            "Epoch: 86, Average Loss 0.027708\n",
            "Training accuracy: 0.9104\n",
            "Epoch: 87, Average Loss 0.028804\n",
            "Training accuracy: 0.9049\n",
            "Epoch: 88, Average Loss 0.029174\n",
            "Training accuracy: 0.9094\n",
            "Epoch: 89, Average Loss 0.026756\n",
            "Training accuracy: 0.9113\n",
            "Epoch: 90, Average Loss 0.027248\n",
            "Training accuracy: 0.9096\n",
            "Epoch: 91, Average Loss 0.029749\n",
            "Training accuracy: 0.8973\n",
            "Epoch: 92, Average Loss 0.030452\n",
            "Training accuracy: 0.8994\n",
            "Epoch: 93, Average Loss 0.027098\n",
            "Training accuracy: 0.9127\n",
            "Epoch: 94, Average Loss 0.024619\n",
            "Training accuracy: 0.9201\n",
            "Epoch: 95, Average Loss 0.027119\n",
            "Training accuracy: 0.9131\n",
            "Epoch: 96, Average Loss 0.024855\n",
            "Training accuracy: 0.9154\n",
            "Epoch: 97, Average Loss 0.026548\n",
            "Training accuracy: 0.9104\n",
            "Epoch: 98, Average Loss 0.030159\n",
            "Training accuracy: 0.8994\n",
            "Epoch: 99, Average Loss 0.028390\n",
            "Training accuracy: 0.9080\n",
            "Epoch: 100, Average Loss 0.021408\n",
            "Training accuracy: 0.9342\n",
            "Epoch: 101, Average Loss 0.015295\n",
            "Training accuracy: 0.9512\n",
            "Epoch: 102, Average Loss 0.012430\n",
            "Training accuracy: 0.9656\n",
            "Epoch: 103, Average Loss 0.012664\n",
            "Training accuracy: 0.9627\n",
            "Epoch: 104, Average Loss 0.010081\n",
            "Training accuracy: 0.9740\n",
            "Epoch: 105, Average Loss 0.009504\n",
            "Training accuracy: 0.9746\n",
            "Epoch: 106, Average Loss 0.010615\n",
            "Training accuracy: 0.9676\n",
            "Epoch: 107, Average Loss 0.007734\n",
            "Training accuracy: 0.9799\n",
            "Epoch: 108, Average Loss 0.007458\n",
            "Training accuracy: 0.9818\n",
            "Epoch: 109, Average Loss 0.007503\n",
            "Training accuracy: 0.9799\n",
            "Epoch: 110, Average Loss 0.006739\n",
            "Training accuracy: 0.9824\n",
            "Epoch: 111, Average Loss 0.006570\n",
            "Training accuracy: 0.9828\n",
            "Epoch: 112, Average Loss 0.005864\n",
            "Training accuracy: 0.9842\n",
            "Epoch: 113, Average Loss 0.005377\n",
            "Training accuracy: 0.9879\n",
            "Epoch: 114, Average Loss 0.005610\n",
            "Training accuracy: 0.9879\n",
            "Epoch: 115, Average Loss 0.005063\n",
            "Training accuracy: 0.9875\n",
            "Epoch: 116, Average Loss 0.004389\n",
            "Training accuracy: 0.9896\n",
            "Epoch: 117, Average Loss 0.005342\n",
            "Training accuracy: 0.9855\n",
            "Epoch: 118, Average Loss 0.004440\n",
            "Training accuracy: 0.9904\n",
            "Epoch: 119, Average Loss 0.003752\n",
            "Training accuracy: 0.9920\n",
            "Epoch: 120, Average Loss 0.003530\n",
            "Training accuracy: 0.9939\n",
            "Epoch: 121, Average Loss 0.003296\n",
            "Training accuracy: 0.9941\n",
            "Epoch: 122, Average Loss 0.003495\n",
            "Training accuracy: 0.9914\n",
            "Epoch: 123, Average Loss 0.002842\n",
            "Training accuracy: 0.9949\n",
            "Epoch: 124, Average Loss 0.002784\n",
            "Training accuracy: 0.9934\n",
            "Epoch: 125, Average Loss 0.002624\n",
            "Training accuracy: 0.9957\n",
            "Epoch: 126, Average Loss 0.002174\n",
            "Training accuracy: 0.9971\n",
            "Epoch: 127, Average Loss 0.002413\n",
            "Training accuracy: 0.9967\n",
            "Epoch: 128, Average Loss 0.002320\n",
            "Training accuracy: 0.9955\n",
            "Epoch: 129, Average Loss 0.002219\n",
            "Training accuracy: 0.9961\n",
            "Epoch: 130, Average Loss 0.002050\n",
            "Training accuracy: 0.9969\n",
            "Epoch: 131, Average Loss 0.001942\n",
            "Training accuracy: 0.9975\n",
            "Epoch: 132, Average Loss 0.001828\n",
            "Training accuracy: 0.9971\n",
            "Epoch: 133, Average Loss 0.001859\n",
            "Training accuracy: 0.9973\n",
            "Epoch: 134, Average Loss 0.001599\n",
            "Training accuracy: 0.9982\n",
            "Epoch: 135, Average Loss 0.001772\n",
            "Training accuracy: 0.9969\n",
            "Epoch: 136, Average Loss 0.001678\n",
            "Training accuracy: 0.9973\n",
            "Epoch: 137, Average Loss 0.001268\n",
            "Training accuracy: 0.9992\n",
            "Epoch: 138, Average Loss 0.001396\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 139, Average Loss 0.001197\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 140, Average Loss 0.001278\n",
            "Training accuracy: 0.9982\n",
            "Epoch: 141, Average Loss 0.001044\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 142, Average Loss 0.001043\n",
            "Training accuracy: 0.9986\n",
            "Epoch: 143, Average Loss 0.000958\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 144, Average Loss 0.001048\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 145, Average Loss 0.000878\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 146, Average Loss 0.000879\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 147, Average Loss 0.000850\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 148, Average Loss 0.000885\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 149, Average Loss 0.000965\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 150, Average Loss 0.000864\n",
            "Training accuracy: 0.9992\n",
            "Epoch: 151, Average Loss 0.000672\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 152, Average Loss 0.000879\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 153, Average Loss 0.000814\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 154, Average Loss 0.000782\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 155, Average Loss 0.000843\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 156, Average Loss 0.000866\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 157, Average Loss 0.000897\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 158, Average Loss 0.000764\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 159, Average Loss 0.000707\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 160, Average Loss 0.000746\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 161, Average Loss 0.000718\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 162, Average Loss 0.000899\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 163, Average Loss 0.000690\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 164, Average Loss 0.000747\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 165, Average Loss 0.000878\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 166, Average Loss 0.000732\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 167, Average Loss 0.000687\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 168, Average Loss 0.000830\n",
            "Training accuracy: 0.9994\n",
            "Epoch: 169, Average Loss 0.000666\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 170, Average Loss 0.000658\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 171, Average Loss 0.000766\n",
            "Training accuracy: 0.9992\n",
            "Epoch: 172, Average Loss 0.000673\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 173, Average Loss 0.000805\n",
            "Training accuracy: 0.9992\n",
            "Epoch: 174, Average Loss 0.000863\n",
            "Training accuracy: 0.9990\n",
            "Epoch: 175, Average Loss 0.000743\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 176, Average Loss 0.000680\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 177, Average Loss 0.000751\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 178, Average Loss 0.000639\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 179, Average Loss 0.000757\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 180, Average Loss 0.000626\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 181, Average Loss 0.000736\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 182, Average Loss 0.000638\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 183, Average Loss 0.000633\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 184, Average Loss 0.000626\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 185, Average Loss 0.000654\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 186, Average Loss 0.000665\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 187, Average Loss 0.000694\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 188, Average Loss 0.000664\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 189, Average Loss 0.000762\n",
            "Training accuracy: 0.9992\n",
            "Epoch: 190, Average Loss 0.000662\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 191, Average Loss 0.000704\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 192, Average Loss 0.000686\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 193, Average Loss 0.000656\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 194, Average Loss 0.000622\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 195, Average Loss 0.000703\n",
            "Training accuracy: 1.0000\n",
            "Epoch: 196, Average Loss 0.000661\n",
            "Training accuracy: 0.9996\n",
            "Epoch: 197, Average Loss 0.000651\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 198, Average Loss 0.000663\n",
            "Training accuracy: 0.9998\n",
            "Epoch: 199, Average Loss 0.000657\n",
            "Training accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.29427647590637207,\n",
              "  0.20280922892148537,\n",
              "  0.18464030146293933,\n",
              "  0.17635644305392603,\n",
              "  0.16830468879026525,\n",
              "  0.15786583435809826,\n",
              "  0.15549855799321324,\n",
              "  0.15074598575796921,\n",
              "  0.1492965187867889,\n",
              "  0.14249504710097446,\n",
              "  0.13892085503434282,\n",
              "  0.13362908881643545,\n",
              "  0.13065442740154998,\n",
              "  0.12244057152277368,\n",
              "  0.12109433537553949,\n",
              "  0.11625369811606834,\n",
              "  0.11315936185514835,\n",
              "  0.11078414496253519,\n",
              "  0.1017987583299427,\n",
              "  0.10529745158636966,\n",
              "  0.1003350371594929,\n",
              "  0.09857528106026027,\n",
              "  0.09424312065934282,\n",
              "  0.0936861612912639,\n",
              "  0.08905610663201803,\n",
              "  0.08878464055488176,\n",
              "  0.0861135792854192,\n",
              "  0.08214228141033436,\n",
              "  0.08217821157801791,\n",
              "  0.075909246721536,\n",
              "  0.0728964168397362,\n",
              "  0.07274077432539762,\n",
              "  0.0722793613553352,\n",
              "  0.07171355092616948,\n",
              "  0.0708716389773142,\n",
              "  0.06477428191458173,\n",
              "  0.06267975709017586,\n",
              "  0.06211421899783337,\n",
              "  0.05982889406516424,\n",
              "  0.05878436138562839,\n",
              "  0.05870236726977941,\n",
              "  0.058892805024486065,\n",
              "  0.054017079303331696,\n",
              "  0.053315643078225956,\n",
              "  0.05337968910746562,\n",
              "  0.0498747970442028,\n",
              "  0.05233485718517352,\n",
              "  0.0456797159693735,\n",
              "  0.049043804559561296,\n",
              "  0.04944997035024111,\n",
              "  0.048859468735087556,\n",
              "  0.04582998568139723,\n",
              "  0.04547170658245721,\n",
              "  0.04330353694193808,\n",
              "  0.04205058960963393,\n",
              "  0.04370172317985378,\n",
              "  0.04152666013259107,\n",
              "  0.04180016038972703,\n",
              "  0.04177023603788117,\n",
              "  0.041716639038242034,\n",
              "  0.03750494713216181,\n",
              "  0.0385806017824451,\n",
              "  0.0394585201197573,\n",
              "  0.03761368033373752,\n",
              "  0.03638537647321706,\n",
              "  0.03662509130090094,\n",
              "  0.036221063312362224,\n",
              "  0.0372517241541382,\n",
              "  0.03327187331741118,\n",
              "  0.03478825843090291,\n",
              "  0.03435311754188879,\n",
              "  0.03221061113088027,\n",
              "  0.03498666438147845,\n",
              "  0.03241009690115214,\n",
              "  0.03255874615953402,\n",
              "  0.030859862942524883,\n",
              "  0.03189501231131346,\n",
              "  0.02940203904953149,\n",
              "  0.033063184079306814,\n",
              "  0.03179764423681342,\n",
              "  0.03245663494252793,\n",
              "  0.028915190788181236,\n",
              "  0.030113836840900313,\n",
              "  0.030060194108797157,\n",
              "  0.029834511182497223,\n",
              "  0.02682914373362461,\n",
              "  0.027707641691807897,\n",
              "  0.028803615580739267,\n",
              "  0.029173740759834914,\n",
              "  0.026755739546492886,\n",
              "  0.027247853138867545,\n",
              "  0.029749432907384986,\n",
              "  0.030452259208845055,\n",
              "  0.027098250815935452,\n",
              "  0.024619089787268577,\n",
              "  0.02711879296223526,\n",
              "  0.024855388590441944,\n",
              "  0.026548226547363163,\n",
              "  0.030158770015782408,\n",
              "  0.028390190523603687,\n",
              "  0.021408042143982696,\n",
              "  0.01529487717868117,\n",
              "  0.012429664125832756,\n",
              "  0.012663850260665044,\n",
              "  0.01008077639409953,\n",
              "  0.009503613890665572,\n",
              "  0.01061452781338521,\n",
              "  0.00773439129047534,\n",
              "  0.007457521460626436,\n",
              "  0.007502549597064552,\n",
              "  0.006738683318390566,\n",
              "  0.006570352324763375,\n",
              "  0.005863957149941293,\n",
              "  0.00537664105024789,\n",
              "  0.005609620231515764,\n",
              "  0.005062874492324527,\n",
              "  0.004389052067304511,\n",
              "  0.005341749171466779,\n",
              "  0.004440008924649957,\n",
              "  0.003751832265359209,\n",
              "  0.0035299388977610854,\n",
              "  0.003295549289192385,\n",
              "  0.003494551057553352,\n",
              "  0.002841730909350583,\n",
              "  0.00278356170419918,\n",
              "  0.002624260728507091,\n",
              "  0.0021742381479429164,\n",
              "  0.0024125809969423374,\n",
              "  0.002320272040546245,\n",
              "  0.0022192681791818204,\n",
              "  0.0020502487940670887,\n",
              "  0.0019417055822966043,\n",
              "  0.001828383138674833,\n",
              "  0.0018592808131233354,\n",
              "  0.0015986428269759163,\n",
              "  0.0017715170848495363,\n",
              "  0.0016781991666845043,\n",
              "  0.0012676513591862242,\n",
              "  0.0013958982732670997,\n",
              "  0.0011967760915665524,\n",
              "  0.0012782861757308932,\n",
              "  0.0010437070656463007,\n",
              "  0.0010434167681123746,\n",
              "  0.000957671832174177,\n",
              "  0.0010476630172737496,\n",
              "  0.0008784761340321635,\n",
              "  0.0008794458313842716,\n",
              "  0.0008498872284922758,\n",
              "  0.0008853654861878937,\n",
              "  0.0009649031047644975,\n",
              "  0.0008641788158375208,\n",
              "  0.0006719141283913341,\n",
              "  0.0008789802445313129,\n",
              "  0.0008144301530259574,\n",
              "  0.0007821400834442786,\n",
              "  0.0008432108538506358,\n",
              "  0.0008658774517467031,\n",
              "  0.0008973809247336272,\n",
              "  0.000764101194670362,\n",
              "  0.0007067353332467625,\n",
              "  0.0007460960711035735,\n",
              "  0.000717511999032572,\n",
              "  0.0008990622358992124,\n",
              "  0.0006899506144244652,\n",
              "  0.0007470319038042632,\n",
              "  0.0008778474478246382,\n",
              "  0.0007319251418380482,\n",
              "  0.00068742365997923,\n",
              "  0.0008304109391958817,\n",
              "  0.0006659253717154798,\n",
              "  0.0006576844662561288,\n",
              "  0.0007663793960714813,\n",
              "  0.0006734393870510409,\n",
              "  0.0008050916958219178,\n",
              "  0.0008631631877282849,\n",
              "  0.0007430399741853595,\n",
              "  0.0006803248191962157,\n",
              "  0.0007506772587576029,\n",
              "  0.000639410941954464,\n",
              "  0.0007570171472676042,\n",
              "  0.0006258977777169794,\n",
              "  0.0007362028939978165,\n",
              "  0.0006377518872547027,\n",
              "  0.0006326788886333518,\n",
              "  0.0006262250738861539,\n",
              "  0.0006542010971194948,\n",
              "  0.0006652534391511889,\n",
              "  0.0006940011947375277,\n",
              "  0.0006639239898718455,\n",
              "  0.0007624016464937983,\n",
              "  0.0006621228888287874,\n",
              "  0.0007039872146999974,\n",
              "  0.0006863528962635323,\n",
              "  0.0006561738931957413,\n",
              "  0.0006221528063811686,\n",
              "  0.0007029626782878738,\n",
              "  0.0006613269099332107,\n",
              "  0.0006505275631080503,\n",
              "  0.0006627141601641846,\n",
              "  0.0006565917765631166],\n",
              " 1.0)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "\n",
        "cost = 0\n",
        "total, correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, label in testset:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        y_pred = NET(data)\n",
        "        # calculate loss \n",
        "        loss = F.cross_entropy(y_pred, label)\n",
        "        cost += loss.item()\n",
        "\n",
        "        total += len(label)\n",
        "        y_hat = torch.argmax(y_pred, dim=1)\n",
        "        correct += torch.sum(torch.eq(y_hat, label)).item()\n",
        "\n",
        "avg_loss_test = cost / len(testset)\n",
        "print(\"The test accuracy is:\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ergaUQBMfSZT",
        "outputId": "b45a2a9b-5a4c-4232-f3a4-7ee30bf8049e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.8606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET = ResNet18_pred(3, Block)\n",
        "CRITERION = nn.CrossEntropyLoss()\n",
        "#CURRENT_LR = 0.0003\n",
        "#CURRENT_LR = 0.001\n",
        "CURRENT_LR = 0.1\n",
        "#OPTIMIZER = optim.Adam(NET.parameters(), lr=CURRENT_LR, weight_decay=1e-4)\n",
        "# optimizer = optim.SGD(net.parameters(), lr=0.3*(BATCH_SIZE / 256), weight_decay=1e-6)\n",
        "OPTIMIZER = optim.SGD(NET.parameters(), lr=CURRENT_LR, momentum=0.9, weight_decay=5e-4)\n",
        "#SCHEDULER = optim.lr_scheduler.StepLR(OPTIMIZER , step_size=80, gamma=0.1)\n",
        "SCHEDULER = torch.optim.lr_scheduler.CosineAnnealingLR(OPTIMIZER, T_max=200)\n",
        "NET.to(device)\n",
        "NET.train()\n",
        "train_epoch_200_epochs_1_p(NET, 200, CRITERION, OPTIMIZER, CURRENT_LR , SCHEDULER, 'test.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBXVw0gUb_BS",
        "outputId": "156ecba4-8b1f-4948-e9a3-6c945d9bc17c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Average Loss 0.034855\n",
            "Training accuracy: 0.1289\n",
            "Epoch: 1, Average Loss 0.044257\n",
            "Training accuracy: 0.1309\n",
            "Epoch: 2, Average Loss 0.035510\n",
            "Training accuracy: 0.1387\n",
            "Epoch: 3, Average Loss 0.030711\n",
            "Training accuracy: 0.1816\n",
            "Epoch: 4, Average Loss 0.025128\n",
            "Training accuracy: 0.2012\n",
            "Epoch: 5, Average Loss 0.026560\n",
            "Training accuracy: 0.1465\n",
            "Epoch: 6, Average Loss 0.022883\n",
            "Training accuracy: 0.2227\n",
            "Epoch: 7, Average Loss 0.022576\n",
            "Training accuracy: 0.1973\n",
            "Epoch: 8, Average Loss 0.021773\n",
            "Training accuracy: 0.2168\n",
            "Epoch: 9, Average Loss 0.021376\n",
            "Training accuracy: 0.2070\n",
            "Epoch: 10, Average Loss 0.021227\n",
            "Training accuracy: 0.2129\n",
            "Epoch: 11, Average Loss 0.020524\n",
            "Training accuracy: 0.2559\n",
            "Epoch: 12, Average Loss 0.021208\n",
            "Training accuracy: 0.2051\n",
            "Epoch: 13, Average Loss 0.020420\n",
            "Training accuracy: 0.2324\n",
            "Epoch: 14, Average Loss 0.020765\n",
            "Training accuracy: 0.2461\n",
            "Epoch: 15, Average Loss 0.021034\n",
            "Training accuracy: 0.2363\n",
            "Epoch: 16, Average Loss 0.020472\n",
            "Training accuracy: 0.2090\n",
            "Epoch: 17, Average Loss 0.021177\n",
            "Training accuracy: 0.2578\n",
            "Epoch: 18, Average Loss 0.020165\n",
            "Training accuracy: 0.2539\n",
            "Epoch: 19, Average Loss 0.020433\n",
            "Training accuracy: 0.2461\n",
            "Epoch: 20, Average Loss 0.019867\n",
            "Training accuracy: 0.2383\n",
            "Epoch: 21, Average Loss 0.019792\n",
            "Training accuracy: 0.2598\n",
            "Epoch: 22, Average Loss 0.018903\n",
            "Training accuracy: 0.3066\n",
            "Epoch: 23, Average Loss 0.019163\n",
            "Training accuracy: 0.2812\n",
            "Epoch: 24, Average Loss 0.019022\n",
            "Training accuracy: 0.3086\n",
            "Epoch: 25, Average Loss 0.018576\n",
            "Training accuracy: 0.2832\n",
            "Epoch: 26, Average Loss 0.018693\n",
            "Training accuracy: 0.3164\n",
            "Epoch: 27, Average Loss 0.018192\n",
            "Training accuracy: 0.3047\n",
            "Epoch: 28, Average Loss 0.018610\n",
            "Training accuracy: 0.3145\n",
            "Epoch: 29, Average Loss 0.018308\n",
            "Training accuracy: 0.2891\n",
            "Epoch: 30, Average Loss 0.017936\n",
            "Training accuracy: 0.3418\n",
            "Epoch: 31, Average Loss 0.018379\n",
            "Training accuracy: 0.2871\n",
            "Epoch: 32, Average Loss 0.017910\n",
            "Training accuracy: 0.3223\n",
            "Epoch: 33, Average Loss 0.016981\n",
            "Training accuracy: 0.3613\n",
            "Epoch: 34, Average Loss 0.017906\n",
            "Training accuracy: 0.3027\n",
            "Epoch: 35, Average Loss 0.017633\n",
            "Training accuracy: 0.3457\n",
            "Epoch: 36, Average Loss 0.016892\n",
            "Training accuracy: 0.3809\n",
            "Epoch: 37, Average Loss 0.017170\n",
            "Training accuracy: 0.3594\n",
            "Epoch: 38, Average Loss 0.016957\n",
            "Training accuracy: 0.3789\n",
            "Epoch: 39, Average Loss 0.017670\n",
            "Training accuracy: 0.3320\n",
            "Epoch: 40, Average Loss 0.017941\n",
            "Training accuracy: 0.3438\n",
            "Epoch: 41, Average Loss 0.017508\n",
            "Training accuracy: 0.3535\n",
            "Epoch: 42, Average Loss 0.017497\n",
            "Training accuracy: 0.3340\n",
            "Epoch: 43, Average Loss 0.017709\n",
            "Training accuracy: 0.3496\n",
            "Epoch: 44, Average Loss 0.017300\n",
            "Training accuracy: 0.3730\n",
            "Epoch: 45, Average Loss 0.017355\n",
            "Training accuracy: 0.3887\n",
            "Epoch: 46, Average Loss 0.016977\n",
            "Training accuracy: 0.4023\n",
            "Epoch: 47, Average Loss 0.016934\n",
            "Training accuracy: 0.3926\n",
            "Epoch: 48, Average Loss 0.017032\n",
            "Training accuracy: 0.3594\n",
            "Epoch: 49, Average Loss 0.016324\n",
            "Training accuracy: 0.4141\n",
            "Epoch: 50, Average Loss 0.016111\n",
            "Training accuracy: 0.4180\n",
            "Epoch: 51, Average Loss 0.016097\n",
            "Training accuracy: 0.4180\n",
            "Epoch: 52, Average Loss 0.016830\n",
            "Training accuracy: 0.3711\n",
            "Epoch: 53, Average Loss 0.016102\n",
            "Training accuracy: 0.4102\n",
            "Epoch: 54, Average Loss 0.015842\n",
            "Training accuracy: 0.3984\n",
            "Epoch: 55, Average Loss 0.015714\n",
            "Training accuracy: 0.4355\n",
            "Epoch: 56, Average Loss 0.016323\n",
            "Training accuracy: 0.4434\n",
            "Epoch: 57, Average Loss 0.015673\n",
            "Training accuracy: 0.4609\n",
            "Epoch: 58, Average Loss 0.015910\n",
            "Training accuracy: 0.4297\n",
            "Epoch: 59, Average Loss 0.015856\n",
            "Training accuracy: 0.3984\n",
            "Epoch: 60, Average Loss 0.015639\n",
            "Training accuracy: 0.4062\n",
            "Epoch: 61, Average Loss 0.015653\n",
            "Training accuracy: 0.4336\n",
            "Epoch: 62, Average Loss 0.016252\n",
            "Training accuracy: 0.4023\n",
            "Epoch: 63, Average Loss 0.016165\n",
            "Training accuracy: 0.4219\n",
            "Epoch: 64, Average Loss 0.016492\n",
            "Training accuracy: 0.3926\n",
            "Epoch: 65, Average Loss 0.015865\n",
            "Training accuracy: 0.4492\n",
            "Epoch: 66, Average Loss 0.015163\n",
            "Training accuracy: 0.4492\n",
            "Epoch: 67, Average Loss 0.016084\n",
            "Training accuracy: 0.4062\n",
            "Epoch: 68, Average Loss 0.015037\n",
            "Training accuracy: 0.4609\n",
            "Epoch: 69, Average Loss 0.014719\n",
            "Training accuracy: 0.4707\n",
            "Epoch: 70, Average Loss 0.016037\n",
            "Training accuracy: 0.4199\n",
            "Epoch: 71, Average Loss 0.015845\n",
            "Training accuracy: 0.4648\n",
            "Epoch: 72, Average Loss 0.015091\n",
            "Training accuracy: 0.4688\n",
            "Epoch: 73, Average Loss 0.015530\n",
            "Training accuracy: 0.4531\n",
            "Epoch: 74, Average Loss 0.015020\n",
            "Training accuracy: 0.4434\n",
            "Epoch: 75, Average Loss 0.015551\n",
            "Training accuracy: 0.4375\n",
            "Epoch: 76, Average Loss 0.014451\n",
            "Training accuracy: 0.4707\n",
            "Epoch: 77, Average Loss 0.014778\n",
            "Training accuracy: 0.5059\n",
            "Epoch: 78, Average Loss 0.014889\n",
            "Training accuracy: 0.4609\n",
            "Epoch: 79, Average Loss 0.015004\n",
            "Training accuracy: 0.4648\n",
            "Epoch: 80, Average Loss 0.015318\n",
            "Training accuracy: 0.4570\n",
            "Epoch: 81, Average Loss 0.015160\n",
            "Training accuracy: 0.4512\n",
            "Epoch: 82, Average Loss 0.015188\n",
            "Training accuracy: 0.4512\n",
            "Epoch: 83, Average Loss 0.014615\n",
            "Training accuracy: 0.4902\n",
            "Epoch: 84, Average Loss 0.015584\n",
            "Training accuracy: 0.4512\n",
            "Epoch: 85, Average Loss 0.014877\n",
            "Training accuracy: 0.4863\n",
            "Epoch: 86, Average Loss 0.014870\n",
            "Training accuracy: 0.4668\n",
            "Epoch: 87, Average Loss 0.014659\n",
            "Training accuracy: 0.4609\n",
            "Epoch: 88, Average Loss 0.014656\n",
            "Training accuracy: 0.4746\n",
            "Epoch: 89, Average Loss 0.014968\n",
            "Training accuracy: 0.4629\n",
            "Epoch: 90, Average Loss 0.014616\n",
            "Training accuracy: 0.4727\n",
            "Epoch: 91, Average Loss 0.014252\n",
            "Training accuracy: 0.5098\n",
            "Epoch: 92, Average Loss 0.014053\n",
            "Training accuracy: 0.4746\n",
            "Epoch: 93, Average Loss 0.013866\n",
            "Training accuracy: 0.4883\n",
            "Epoch: 94, Average Loss 0.013847\n",
            "Training accuracy: 0.5137\n",
            "Epoch: 95, Average Loss 0.013867\n",
            "Training accuracy: 0.4922\n",
            "Epoch: 96, Average Loss 0.013795\n",
            "Training accuracy: 0.4902\n",
            "Epoch: 97, Average Loss 0.014210\n",
            "Training accuracy: 0.5020\n",
            "Epoch: 98, Average Loss 0.014795\n",
            "Training accuracy: 0.4824\n",
            "Epoch: 99, Average Loss 0.013828\n",
            "Training accuracy: 0.4980\n",
            "Epoch: 100, Average Loss 0.014114\n",
            "Training accuracy: 0.4746\n",
            "Epoch: 101, Average Loss 0.013671\n",
            "Training accuracy: 0.5078\n",
            "Epoch: 102, Average Loss 0.012225\n",
            "Training accuracy: 0.5781\n",
            "Epoch: 103, Average Loss 0.012766\n",
            "Training accuracy: 0.5312\n",
            "Epoch: 104, Average Loss 0.013115\n",
            "Training accuracy: 0.5156\n",
            "Epoch: 105, Average Loss 0.012354\n",
            "Training accuracy: 0.5352\n",
            "Epoch: 106, Average Loss 0.013415\n",
            "Training accuracy: 0.5293\n",
            "Epoch: 107, Average Loss 0.013303\n",
            "Training accuracy: 0.5273\n",
            "Epoch: 108, Average Loss 0.012170\n",
            "Training accuracy: 0.5762\n",
            "Epoch: 109, Average Loss 0.012762\n",
            "Training accuracy: 0.5273\n",
            "Epoch: 110, Average Loss 0.013304\n",
            "Training accuracy: 0.5176\n",
            "Epoch: 111, Average Loss 0.012242\n",
            "Training accuracy: 0.5352\n",
            "Epoch: 112, Average Loss 0.012608\n",
            "Training accuracy: 0.5527\n",
            "Epoch: 113, Average Loss 0.013382\n",
            "Training accuracy: 0.5137\n",
            "Epoch: 114, Average Loss 0.012722\n",
            "Training accuracy: 0.5645\n",
            "Epoch: 115, Average Loss 0.012919\n",
            "Training accuracy: 0.5488\n",
            "Epoch: 116, Average Loss 0.012687\n",
            "Training accuracy: 0.5586\n",
            "Epoch: 117, Average Loss 0.012385\n",
            "Training accuracy: 0.5605\n",
            "Epoch: 118, Average Loss 0.012622\n",
            "Training accuracy: 0.5605\n",
            "Epoch: 119, Average Loss 0.012356\n",
            "Training accuracy: 0.5508\n",
            "Epoch: 120, Average Loss 0.012189\n",
            "Training accuracy: 0.5723\n",
            "Epoch: 121, Average Loss 0.012174\n",
            "Training accuracy: 0.5527\n",
            "Epoch: 122, Average Loss 0.012630\n",
            "Training accuracy: 0.5293\n",
            "Epoch: 123, Average Loss 0.012456\n",
            "Training accuracy: 0.5508\n",
            "Epoch: 124, Average Loss 0.012845\n",
            "Training accuracy: 0.5332\n",
            "Epoch: 125, Average Loss 0.011976\n",
            "Training accuracy: 0.5723\n",
            "Epoch: 126, Average Loss 0.012505\n",
            "Training accuracy: 0.5723\n",
            "Epoch: 127, Average Loss 0.012301\n",
            "Training accuracy: 0.5820\n",
            "Epoch: 128, Average Loss 0.012658\n",
            "Training accuracy: 0.5469\n",
            "Epoch: 129, Average Loss 0.012601\n",
            "Training accuracy: 0.5781\n",
            "Epoch: 130, Average Loss 0.012147\n",
            "Training accuracy: 0.5742\n",
            "Epoch: 131, Average Loss 0.012146\n",
            "Training accuracy: 0.5488\n",
            "Epoch: 132, Average Loss 0.012232\n",
            "Training accuracy: 0.5645\n",
            "Epoch: 133, Average Loss 0.011626\n",
            "Training accuracy: 0.6172\n",
            "Epoch: 134, Average Loss 0.011748\n",
            "Training accuracy: 0.5898\n",
            "Epoch: 135, Average Loss 0.012035\n",
            "Training accuracy: 0.5566\n",
            "Epoch: 136, Average Loss 0.011076\n",
            "Training accuracy: 0.5957\n",
            "Epoch: 137, Average Loss 0.012990\n",
            "Training accuracy: 0.5352\n",
            "Epoch: 138, Average Loss 0.011573\n",
            "Training accuracy: 0.5996\n",
            "Epoch: 139, Average Loss 0.012061\n",
            "Training accuracy: 0.5547\n",
            "Epoch: 140, Average Loss 0.012782\n",
            "Training accuracy: 0.5527\n",
            "Epoch: 141, Average Loss 0.012077\n",
            "Training accuracy: 0.5488\n",
            "Epoch: 142, Average Loss 0.011844\n",
            "Training accuracy: 0.5449\n",
            "Epoch: 143, Average Loss 0.011587\n",
            "Training accuracy: 0.5977\n",
            "Epoch: 144, Average Loss 0.011414\n",
            "Training accuracy: 0.5723\n",
            "Epoch: 145, Average Loss 0.011284\n",
            "Training accuracy: 0.5801\n",
            "Epoch: 146, Average Loss 0.011621\n",
            "Training accuracy: 0.5762\n",
            "Epoch: 147, Average Loss 0.011511\n",
            "Training accuracy: 0.5781\n",
            "Epoch: 148, Average Loss 0.012610\n",
            "Training accuracy: 0.5742\n",
            "Epoch: 149, Average Loss 0.012457\n",
            "Training accuracy: 0.5508\n",
            "Epoch: 150, Average Loss 0.011789\n",
            "Training accuracy: 0.5664\n",
            "Epoch: 151, Average Loss 0.013031\n",
            "Training accuracy: 0.5449\n",
            "Epoch: 152, Average Loss 0.011408\n",
            "Training accuracy: 0.5742\n",
            "Epoch: 153, Average Loss 0.012096\n",
            "Training accuracy: 0.5508\n",
            "Epoch: 154, Average Loss 0.012232\n",
            "Training accuracy: 0.5645\n",
            "Epoch: 155, Average Loss 0.011751\n",
            "Training accuracy: 0.5977\n",
            "Epoch: 156, Average Loss 0.011508\n",
            "Training accuracy: 0.5938\n",
            "Epoch: 157, Average Loss 0.011287\n",
            "Training accuracy: 0.6172\n",
            "Epoch: 158, Average Loss 0.012383\n",
            "Training accuracy: 0.5566\n",
            "Epoch: 159, Average Loss 0.011356\n",
            "Training accuracy: 0.6074\n",
            "Epoch: 160, Average Loss 0.011777\n",
            "Training accuracy: 0.5938\n",
            "Epoch: 161, Average Loss 0.012575\n",
            "Training accuracy: 0.5586\n",
            "Epoch: 162, Average Loss 0.011906\n",
            "Training accuracy: 0.5645\n",
            "Epoch: 163, Average Loss 0.011834\n",
            "Training accuracy: 0.5938\n",
            "Epoch: 164, Average Loss 0.011182\n",
            "Training accuracy: 0.5801\n",
            "Epoch: 165, Average Loss 0.011235\n",
            "Training accuracy: 0.5977\n",
            "Epoch: 166, Average Loss 0.011267\n",
            "Training accuracy: 0.6055\n",
            "Epoch: 167, Average Loss 0.011990\n",
            "Training accuracy: 0.5781\n",
            "Epoch: 168, Average Loss 0.011891\n",
            "Training accuracy: 0.5566\n",
            "Epoch: 169, Average Loss 0.011314\n",
            "Training accuracy: 0.5840\n",
            "Epoch: 170, Average Loss 0.011399\n",
            "Training accuracy: 0.6016\n",
            "Epoch: 171, Average Loss 0.011907\n",
            "Training accuracy: 0.5352\n",
            "Epoch: 172, Average Loss 0.011193\n",
            "Training accuracy: 0.6191\n",
            "Epoch: 173, Average Loss 0.011430\n",
            "Training accuracy: 0.6230\n",
            "Epoch: 174, Average Loss 0.011397\n",
            "Training accuracy: 0.5977\n",
            "Epoch: 175, Average Loss 0.012058\n",
            "Training accuracy: 0.5879\n",
            "Epoch: 176, Average Loss 0.011231\n",
            "Training accuracy: 0.6055\n",
            "Epoch: 177, Average Loss 0.011189\n",
            "Training accuracy: 0.6035\n",
            "Epoch: 178, Average Loss 0.012132\n",
            "Training accuracy: 0.5859\n",
            "Epoch: 179, Average Loss 0.011235\n",
            "Training accuracy: 0.5977\n",
            "Epoch: 180, Average Loss 0.011791\n",
            "Training accuracy: 0.5938\n",
            "Epoch: 181, Average Loss 0.011684\n",
            "Training accuracy: 0.5664\n",
            "Epoch: 182, Average Loss 0.011761\n",
            "Training accuracy: 0.5625\n",
            "Epoch: 183, Average Loss 0.010877\n",
            "Training accuracy: 0.6094\n",
            "Epoch: 184, Average Loss 0.011651\n",
            "Training accuracy: 0.5879\n",
            "Epoch: 185, Average Loss 0.011486\n",
            "Training accuracy: 0.6016\n",
            "Epoch: 186, Average Loss 0.011801\n",
            "Training accuracy: 0.5684\n",
            "Epoch: 187, Average Loss 0.012036\n",
            "Training accuracy: 0.5762\n",
            "Epoch: 188, Average Loss 0.011927\n",
            "Training accuracy: 0.5625\n",
            "Epoch: 189, Average Loss 0.011382\n",
            "Training accuracy: 0.6094\n",
            "Epoch: 190, Average Loss 0.011882\n",
            "Training accuracy: 0.5762\n",
            "Epoch: 191, Average Loss 0.011250\n",
            "Training accuracy: 0.6113\n",
            "Epoch: 192, Average Loss 0.010815\n",
            "Training accuracy: 0.6113\n",
            "Epoch: 193, Average Loss 0.011237\n",
            "Training accuracy: 0.6328\n",
            "Epoch: 194, Average Loss 0.011449\n",
            "Training accuracy: 0.6016\n",
            "Epoch: 195, Average Loss 0.012534\n",
            "Training accuracy: 0.5625\n",
            "Epoch: 196, Average Loss 0.012231\n",
            "Training accuracy: 0.5801\n",
            "Epoch: 197, Average Loss 0.010981\n",
            "Training accuracy: 0.6074\n",
            "Epoch: 198, Average Loss 0.011197\n",
            "Training accuracy: 0.6035\n",
            "Epoch: 199, Average Loss 0.010889\n",
            "Training accuracy: 0.6367\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([0.034854983429774604,\n",
              "  0.04425675484835339,\n",
              "  0.035509950974408316,\n",
              "  0.030711034374773655,\n",
              "  0.025128341391873178,\n",
              "  0.026559578488245035,\n",
              "  0.022883231682545694,\n",
              "  0.022576261969173655,\n",
              "  0.021772868797907134,\n",
              "  0.021375668018370333,\n",
              "  0.021227213427843644,\n",
              "  0.02052400423132855,\n",
              "  0.021208393604249295,\n",
              "  0.020419705554347516,\n",
              "  0.020765319199818173,\n",
              "  0.021033801386118545,\n",
              "  0.020471833551021487,\n",
              "  0.021176555882329525,\n",
              "  0.02016517055004149,\n",
              "  0.020432790831836592,\n",
              "  0.01986670158708187,\n",
              "  0.019792400052785264,\n",
              "  0.01890321918155836,\n",
              "  0.01916274908558487,\n",
              "  0.01902224980961636,\n",
              "  0.018576084195500445,\n",
              "  0.01869254404931422,\n",
              "  0.018192134244972482,\n",
              "  0.018610048172114146,\n",
              "  0.018308194092167614,\n",
              "  0.017936443428859077,\n",
              "  0.018379262341257863,\n",
              "  0.01790996189312557,\n",
              "  0.01698099896121208,\n",
              "  0.01790550572183126,\n",
              "  0.017633208533382173,\n",
              "  0.016891629799552586,\n",
              "  0.017170138676148243,\n",
              "  0.016957248873113062,\n",
              "  0.017670464942522366,\n",
              "  0.017940806915692966,\n",
              "  0.017508143049371823,\n",
              "  0.01749684194774579,\n",
              "  0.017708803686644414,\n",
              "  0.017300435954042712,\n",
              "  0.017355135639610192,\n",
              "  0.016976598888406972,\n",
              "  0.01693415397878193,\n",
              "  0.017031988219531904,\n",
              "  0.016323518875004994,\n",
              "  0.016110995846331272,\n",
              "  0.016096754757034808,\n",
              "  0.016830251040056234,\n",
              "  0.016102036856629354,\n",
              "  0.015841760293906913,\n",
              "  0.015713515184114655,\n",
              "  0.01632275300867417,\n",
              "  0.01567302609953429,\n",
              "  0.01590956598901383,\n",
              "  0.0158559580898041,\n",
              "  0.015639387738064427,\n",
              "  0.015652853204771076,\n",
              "  0.01625183933531232,\n",
              "  0.016164618989695675,\n",
              "  0.01649160519280397,\n",
              "  0.015864814333903514,\n",
              "  0.015162892353809094,\n",
              "  0.01608362588126336,\n",
              "  0.015036524714106488,\n",
              "  0.01471853713550226,\n",
              "  0.016036802850415945,\n",
              "  0.01584474753845683,\n",
              "  0.015091467696382567,\n",
              "  0.015529684398485266,\n",
              "  0.015020261945017158,\n",
              "  0.015550968287241123,\n",
              "  0.014451303445469693,\n",
              "  0.014778399406491644,\n",
              "  0.014888791172095881,\n",
              "  0.01500436381610763,\n",
              "  0.015317773270180158,\n",
              "  0.015160172796615249,\n",
              "  0.015188050696916897,\n",
              "  0.014614948836129035,\n",
              "  0.015584113347865736,\n",
              "  0.01487736049515512,\n",
              "  0.014870356110965504,\n",
              "  0.014659423657390468,\n",
              "  0.01465602664996291,\n",
              "  0.014968092179359377,\n",
              "  0.014616326907711566,\n",
              "  0.01425155044516639,\n",
              "  0.014053159662524758,\n",
              "  0.013865635523100947,\n",
              "  0.013846619354794398,\n",
              "  0.01386710658402699,\n",
              "  0.013795059965089763,\n",
              "  0.014210338482771383,\n",
              "  0.014794748762379522,\n",
              "  0.013828442529644198,\n",
              "  0.014113752128523024,\n",
              "  0.013671267672877787,\n",
              "  0.012224801361103496,\n",
              "  0.012766389590699959,\n",
              "  0.013114683768328498,\n",
              "  0.012354319967577219,\n",
              "  0.013414660987951566,\n",
              "  0.01330278535633136,\n",
              "  0.012170118139223064,\n",
              "  0.01276161878005318,\n",
              "  0.013304435383633274,\n",
              "  0.01224218213649662,\n",
              "  0.012607598243771917,\n",
              "  0.01338168635697621,\n",
              "  0.012722407460517591,\n",
              "  0.012918613755794438,\n",
              "  0.012687325172717005,\n",
              "  0.012384587236682473,\n",
              "  0.012622497880550297,\n",
              "  0.012356071520949264,\n",
              "  0.01218858185936423,\n",
              "  0.01217383832272971,\n",
              "  0.012630198312842327,\n",
              "  0.012455652132058693,\n",
              "  0.012844597592073329,\n",
              "  0.011976057306275038,\n",
              "  0.012504598063886013,\n",
              "  0.012300669079851312,\n",
              "  0.012657876819600839,\n",
              "  0.012600854229744133,\n",
              "  0.012147145807895514,\n",
              "  0.012146017740449638,\n",
              "  0.012231889435702272,\n",
              "  0.011626182614689898,\n",
              "  0.011747722125724148,\n",
              "  0.012034850047372491,\n",
              "  0.011076415591227734,\n",
              "  0.012989618284318149,\n",
              "  0.011573011612953127,\n",
              "  0.012061210239634794,\n",
              "  0.012781800821309199,\n",
              "  0.012077104405064107,\n",
              "  0.01184388469247257,\n",
              "  0.011586967331674093,\n",
              "  0.011413981237679796,\n",
              "  0.01128416537018993,\n",
              "  0.011620879478161901,\n",
              "  0.011511171565336339,\n",
              "  0.012610294020084468,\n",
              "  0.012457031423173597,\n",
              "  0.011789303606428454,\n",
              "  0.013030560730058519,\n",
              "  0.011408087847482823,\n",
              "  0.012096318137615233,\n",
              "  0.012231768244672614,\n",
              "  0.011751034832976359,\n",
              "  0.011508200174707281,\n",
              "  0.011287099412640037,\n",
              "  0.012382999405531628,\n",
              "  0.011356013205350207,\n",
              "  0.011776566200549036,\n",
              "  0.01257497148440622,\n",
              "  0.011906350055314086,\n",
              "  0.01183384702638592,\n",
              "  0.011182470089944123,\n",
              "  0.011234558756699037,\n",
              "  0.011267053501685257,\n",
              "  0.011989614847675918,\n",
              "  0.011890848884192269,\n",
              "  0.011314407181556877,\n",
              "  0.011399147760532701,\n",
              "  0.011906992139108955,\n",
              "  0.011193297708125981,\n",
              "  0.011429584849521023,\n",
              "  0.011396721195991692,\n",
              "  0.012058436718133404,\n",
              "  0.011230685826762558,\n",
              "  0.011189410143801013,\n",
              "  0.012132427576557754,\n",
              "  0.01123474199143822,\n",
              "  0.011791257907057663,\n",
              "  0.01168396344880009,\n",
              "  0.011760685145092742,\n",
              "  0.010876565333217611,\n",
              "  0.01165059186003702,\n",
              "  0.011486469327336383,\n",
              "  0.011800640379376424,\n",
              "  0.012036441842003552,\n",
              "  0.011927327231677902,\n",
              "  0.011382445807347213,\n",
              "  0.011881814283483168,\n",
              "  0.011249954743153604,\n",
              "  0.010815408223730219,\n",
              "  0.011236590192750896,\n",
              "  0.011448737148128812,\n",
              "  0.012534394288611838,\n",
              "  0.0122314228121277,\n",
              "  0.01098100273200618,\n",
              "  0.01119675630193842,\n",
              "  0.010889332007874003],\n",
              " 0.63671875)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NET.eval()\n",
        "\n",
        "cost = 0\n",
        "total, correct = 0, 0\n",
        "with torch.no_grad():\n",
        "    for data, label in testset:\n",
        "        data, label = data.to(device), label.to(device)\n",
        "        y_pred = NET(data)\n",
        "        # calculate loss \n",
        "        loss = F.cross_entropy(y_pred, label)\n",
        "        cost += loss.item()\n",
        "\n",
        "        total += len(label)\n",
        "        y_hat = torch.argmax(y_pred, dim=1)\n",
        "        correct += torch.sum(torch.eq(y_hat, label)).item()\n",
        "\n",
        "avg_loss_test = cost / len(testset)\n",
        "print(\"The test accuracy is:\", correct/total)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI7E_XEKdRt_",
        "outputId": "f8cd9bd5-1441-449c-d8cd-02cba5053384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The test accuracy is: 0.5789\n"
          ]
        }
      ]
    }
  ]
}